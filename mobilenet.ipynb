{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea9a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23575 files belonging to 2 classes.\n",
      "Train samples:      1451     batches(13) ==> 18863\n",
      "Validation samples: 272       batches(13) ==> 3536\n",
      "Test samples:       91      batches(13) ==> 1183\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data_dir = 'C:/Users/Anne/OneDrive - National University of Ireland, Galway/Documents/Data Analytics/PROJECT/Capstone2025_Anne/kaggle/working/merged_images'  # Update with the dataset path\n",
    "\n",
    "# Create a dataset for the entire data to use for split\n",
    "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    # image_size=(224, 224),\n",
    "    image_size=(224, 224),\n",
    "    seed=50,\n",
    "    shuffle=True,\n",
    "    batch_size=13\n",
    ")\n",
    "# Calculate the total number of samples\n",
    "total_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "\n",
    "train_size = int(0.8 * total_samples)                 # 70% for training\n",
    "val_size   = int(0.15 * total_samples)                # 20% for validation\n",
    "test_size = total_samples - train_size - val_size     # 10% for testing\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset       = full_dataset.take(train_size)\n",
    "validation_dataset  = full_dataset.skip(train_size).take(val_size)\n",
    "test_dataset        = full_dataset.skip(train_size + val_size)\n",
    "\n",
    "train_dataset      = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(f\"Train samples:      {train_size}     batches(13) ==> {train_size*13}\")\n",
    "print(f\"Validation samples: {val_size}       batches(13) ==> {val_size*13}\")\n",
    "print(f\"Test samples:       {test_size}      batches(13) ==> {test_size*13}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ceebbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "def try_model():\n",
    "    # Load the base model with MobileNet\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Calculate the index to start unfreezing layers\n",
    "    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - 50.0 / 100.0)))\n",
    "\n",
    "    # Unfreeze layers from the calculated index onwards\n",
    "    for layer in base_model.layers[from_index:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Add custom layers on top (Upper-Layers)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "trymodel = try_model()\n",
    "\n",
    "# Compile the model\n",
    "trymodel.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "history_V = trymodel.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    batch_size=13,\n",
    "    epochs=7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95807b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trymodel.save(\"MobileNet.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the validation loss\n",
    "plt.plot(history_V.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history_V.history['loss'], label='Training Loss')\n",
    "plt.title('Validation and Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation accuracy\n",
    "plt.plot(history_V.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(history_V.history['accuracy'], label='Training Accuracy')\n",
    "plt.title('Validation and Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff568a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# --- Load model ---\n",
    "model = tf.keras.models.load_model(\"Resnet50_v3.keras\")  # <- Update path\n",
    "\n",
    "# --- Base directory ---\n",
    "base_dir = r\"C:\\Users\\Anne\\OneDrive - National University of Ireland, Galway\\Documents\\Data Analytics\\PROJECT\\Capstone2025_Anne\\kaggle\\working\\merged_images\"\n",
    "\n",
    "# --- Parameters ---\n",
    "img_size = (224, 224)\n",
    "num_samples = 12\n",
    "\n",
    "# --- OPTIONAL: Uncomment for reproducible results\n",
    "# random.seed(42)\n",
    "\n",
    "# --- Collect image paths and labels ---\n",
    "image_paths, labels = [], []\n",
    "for label_folder in [\"0\", \"1\"]:\n",
    "    folder_path = os.path.join(base_dir, label_folder)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_paths.append(os.path.join(folder_path, fname))\n",
    "            labels.append(int(label_folder))\n",
    "\n",
    "# --- Random sample ---\n",
    "sample = random.sample(list(zip(image_paths, labels)), num_samples)\n",
    "sample_paths, sample_labels = zip(*sample)\n",
    "\n",
    "# --- Preprocessing + Prediction ---\n",
    "def preprocess(img_path):\n",
    "    img = image.load_img(img_path, target_size=img_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
    "    return np.expand_dims(img_array, axis=0), img\n",
    "\n",
    "predictions = []\n",
    "original_imgs = []\n",
    "\n",
    "for img_path in sample_paths:\n",
    "    input_arr, orig = preprocess(img_path)\n",
    "    pred = model.predict(input_arr, verbose=0)[0][0]\n",
    "    predictions.append(pred)\n",
    "    original_imgs.append(orig)\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.imshow(original_imgs[i])\n",
    "    actual = \"Malignant\" if sample_labels[i] == 1 else \"Benign\"\n",
    "    predicted = \"Malignant\" if predictions[i] < 0.5 else \"Benign\"\n",
    "    confidence = f\"{predictions[i]:.2f}\"\n",
    "    title_color = 'green' if actual == predicted else 'red'\n",
    "    plt.title(f\"Actual: {actual}\\nPredicted: {predicted} ({confidence})\", color=title_color)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
