{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f306aa",
   "metadata": {},
   "source": [
    "# Classification Performance Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95328691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17353 files belonging to 2 classes.\n",
      "Test samples:       67      batches(13) ==> 871\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "data_dir = r'C:\\Users\\Anne\\OneDrive - National University of Ireland, Galway\\Documents\\Data Analytics\\PROJECT\\Capstone2025_Anne\\cbis_ddsm_dataset\\merged_data'\n",
    "\n",
    "\n",
    "# Create a dataset for the entire data to use for split\n",
    "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    # image_size=(224, 224),\n",
    "    image_size=(224, 224),\n",
    "    seed=50,\n",
    "    shuffle=True,\n",
    "    batch_size=13\n",
    ")\n",
    "# Calculate the total number of samples\n",
    "total_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "\n",
    "train_size = int(0.8 * total_samples)                 # 70% for training\n",
    "val_size   = int(0.15 * total_samples)                # 20% for validation\n",
    "test_size = total_samples - train_size - val_size     # 10% for testing\n",
    "\n",
    "# test set\n",
    "test_dataset        = full_dataset.skip(train_size + val_size)\n",
    "\n",
    "test_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(f\"Test samples:       {test_size}      batches(13) ==> {test_size*13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b096e56",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f847e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      0.91      0.95       467\n",
      "   Malignant       0.91      0.99      0.95       402\n",
      "\n",
      "    accuracy                           0.95       869\n",
      "   macro avg       0.95      0.95      0.95       869\n",
      "weighted avg       0.95      0.95      0.95       869\n",
      "\n",
      "Confusion Matrix:\n",
      " [[426  41]\n",
      " [  5 397]]\n",
      "Accuracy: 0.9471\n",
      "Precision: 0.9064\n",
      "Recall: 0.9876\n",
      "F1 Score: 0.9452\n",
      "AUC Score: 0.9916\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 870ms/step - accuracy: 0.9388 - loss: 0.2345 - precision: 0.9388 - recall: 0.9388\n",
      "Loss: 0.1858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load model ---\n",
    "model = tf.keras.models.load_model(\"trained_model/ResNet50.keras\")  \n",
    "\n",
    "# --- Predictions ---\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []  # for AUC\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = np.argmax(labels.numpy(), axis=1)\n",
    "    \n",
    "    y_pred.extend(pred_classes)\n",
    "    y_true.extend(true_classes)\n",
    "    y_prob.extend(preds[:, 1])  # Probability for class 1 (Malignant)\n",
    "\n",
    "# --- Calculate and print metrics ---\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Classification Report on Test Set:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malignant\"]))\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Optional: also evaluate with Keras built-in method\n",
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "loss = results[0]  # first element is loss\n",
    "print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf1f5f",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbe91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.94      0.96      0.95       474\n",
      "   Malignant       0.95      0.92      0.93       395\n",
      "\n",
      "    accuracy                           0.94       869\n",
      "   macro avg       0.94      0.94      0.94       869\n",
      "weighted avg       0.94      0.94      0.94       869\n",
      "\n",
      "Confusion Matrix:\n",
      " [[453  21]\n",
      " [ 30 365]]\n",
      "Accuracy: 0.9413\n",
      "Precision: 0.9456\n",
      "Recall: 0.9241\n",
      "F1 Score: 0.9347\n",
      "AUC Score: 0.9862\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 637ms/step - accuracy: 0.9369 - loss: 0.1832 - precision_v2: 0.9369 - recall_v2: 0.9369\n",
      "Loss: 0.1912\n"
     ]
    }
   ],
   "source": [
    "# --- Load model ---\n",
    "model = tf.keras.models.load_model(\"trained_model/ResNet50_v2.keras\")  \n",
    "\n",
    "\n",
    "# --- Predictions ---\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []  # for AUC\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = np.argmax(labels.numpy(), axis=1)\n",
    "    \n",
    "    y_pred.extend(pred_classes)\n",
    "    y_true.extend(true_classes)\n",
    "    y_prob.extend(preds[:, 1])  # Probability for class 1 (Malignant)\n",
    "\n",
    "# --- Calculate and print metrics ---\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Classification Report on Test Set:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malignant\"]))\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Optional: also evaluate with Keras built-in method\n",
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "loss = results[0]  # first element is loss\n",
    "print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ed9fa",
   "metadata": {},
   "source": [
    "## V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace54094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.97      0.88      0.92       473\n",
      "   Malignant       0.87      0.97      0.92       396\n",
      "\n",
      "    accuracy                           0.92       869\n",
      "   macro avg       0.92      0.92      0.92       869\n",
      "weighted avg       0.93      0.92      0.92       869\n",
      "\n",
      "Confusion Matrix:\n",
      " [[416  57]\n",
      " [ 12 384]]\n",
      "Accuracy: 0.9206\n",
      "Precision: 0.8707\n",
      "Recall: 0.9697\n",
      "F1 Score: 0.9176\n",
      "AUC Score: 0.9688\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 659ms/step - accuracy: 0.9214 - loss: 0.7063 - precision_v3: 0.9214 - recall_v3: 0.9214\n",
      "Loss: 0.7424\n"
     ]
    }
   ],
   "source": [
    "# --- Load model ---\n",
    "model = tf.keras.models.load_model(\"trained_model/ResNet50_v3.keras\")  \n",
    "\n",
    "# --- Predictions ---\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []  # for AUC\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = np.argmax(labels.numpy(), axis=1)\n",
    "    \n",
    "    y_pred.extend(pred_classes)\n",
    "    y_true.extend(true_classes)\n",
    "    y_prob.extend(preds[:, 1])  # Probability for class 1 (Malignant)\n",
    "\n",
    "# --- Calculate and print metrics ---\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Classification Report on Test Set:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malignant\"]))\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Optional: also evaluate with Keras built-in method\n",
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "loss = results[0]  # first element is loss\n",
    "print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb66379",
   "metadata": {},
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.98      0.80      0.88       472\n",
      "   Malignant       0.80      0.98      0.88       397\n",
      "\n",
      "    accuracy                           0.88       869\n",
      "   macro avg       0.89      0.89      0.88       869\n",
      "weighted avg       0.90      0.88      0.88       869\n",
      "\n",
      "Confusion Matrix:\n",
      " [[377  95]\n",
      " [  9 388]]\n",
      "Accuracy: 0.8803\n",
      "Precision: 0.8033\n",
      "Recall: 0.9773\n",
      "F1 Score: 0.8818\n",
      "AUC Score: 0.9705\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 183ms/step - accuracy: 0.8812 - loss: 0.5188 - precision: 0.8812 - recall: 0.8812\n",
      "Loss: 0.4213\n"
     ]
    }
   ],
   "source": [
    "# --- Load model ---\n",
    "model = tf.keras.models.load_model(\"trained_model/MobileNet.keras\")  \n",
    "\n",
    "# --- Predictions ---\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []  # for AUC\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = np.argmax(labels.numpy(), axis=1)\n",
    "    \n",
    "    y_pred.extend(pred_classes)\n",
    "    y_true.extend(true_classes)\n",
    "    y_prob.extend(preds[:, 1])  # Probability for class 1 (Malignant)\n",
    "    \n",
    "# --- Calculate and print metrics ---\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Classification Report on Test Set:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malignant\"]))\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Optional: also evaluate with Keras built-in method\n",
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "loss = results[0]  # first element is loss\n",
    "print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a0376",
   "metadata": {},
   "source": [
    "## vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa88c407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Test Set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.92      0.94      0.93       469\n",
      "   Malignant       0.93      0.90      0.92       400\n",
      "\n",
      "    accuracy                           0.92       869\n",
      "   macro avg       0.92      0.92      0.92       869\n",
      "weighted avg       0.92      0.92      0.92       869\n",
      "\n",
      "Confusion Matrix:\n",
      " [[442  27]\n",
      " [ 39 361]]\n",
      "Accuracy: 0.9241\n",
      "Precision: 0.9304\n",
      "Recall: 0.9025\n",
      "F1 Score: 0.9162\n",
      "AUC Score: 0.9739\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.9094 - loss: 0.2604 - precision: 0.9094 - recall: 0.9094\n",
      "Loss: 0.2291\n"
     ]
    }
   ],
   "source": [
    "# --- Load model ---\n",
    "model = tf.keras.models.load_model(\"trained_model/vgg16.keras\")  \n",
    "\n",
    "# --- Predictions ---\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []  # for AUC\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = np.argmax(labels.numpy(), axis=1)\n",
    "    \n",
    "    y_pred.extend(pred_classes)\n",
    "    y_true.extend(true_classes)\n",
    "    y_prob.extend(preds[:, 1])  # Probability for class 1 (Malignant)\n",
    "\n",
    "# --- Calculate and print metrics ---\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Classification Report on Test Set:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Malignant\"]))\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Optional: also evaluate with Keras built-in method\n",
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "loss = results[0]  # first element is loss\n",
    "print(f\"Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822416c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
